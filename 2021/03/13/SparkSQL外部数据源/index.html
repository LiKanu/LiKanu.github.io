<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	<!--script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script-->
	<!--link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"-->
	<script type="text/javascript" src="/js/pace.min.js"></script> 
	<!--script async src="/js/busuanzi.pure.mini.js"></script-->
    <meta name="description" content="Spark SQL 外部数据源 一、简介 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.1 多数据源支持 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.2 读数据格式 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.3 写数据格式 二、CS">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkSQL外部数据源">
<meta property="og:url" content="https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/index.html">
<meta property="og:site_name" content="Likanug&#39;s Blog">
<meta property="og:description" content="Spark SQL 外部数据源 一、简介 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.1 多数据源支持 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.2 读数据格式 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;1.3 写数据格式 二、CS">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-mysql-分区上下限.png">
<meta property="og:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-分区.png">
<meta property="article:published_time" content="2021-03-13T10:31:55.000Z">
<meta property="article:modified_time" content="2021-03-13T11:35:51.175Z">
<meta property="article:author" content="Likanug">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-mysql-分区上下限.png">
    
    
        
          
              <link rel="shortcut icon" href="../../../../images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="../../../../images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="../../../../images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>SparkSQL外部数据源</title>
    <!-- styles -->
    
<link rel="stylesheet" href="../../../../css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="../../../../css/rtl.css">

    
    <!-- rss -->
    
    
    <!-- styles -->

<link rel="stylesheet" href="../../../../lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="../../../../lib/justified-gallery/css/justifiedGallery.min.css">


<meta name="generator" content="Hexo 5.0.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="../../../../index.html">Home</a></li>
         
          <li><a href="../../../../archives/">Writing</a></li>
         
          <li><a href="../../../../about/">About</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="../Kafka%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="../Spark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&text=SparkSQL外部数据源"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&is_video=false&description=SparkSQL外部数据源"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=SparkSQL外部数据源&body=Check out this article: https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&name=SparkSQL外部数据源&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&t=SparkSQL外部数据源"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL-%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL 外部数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">一、简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%94%AF%E6%8C%81"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 多数据源支持</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 读数据格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%86%99%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 写数据格式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81CSV"><span class="toc-number">1.2.</span> <span class="toc-text">二、CSV</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%BB%E5%8F%96CSV%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 读取CSV文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%86%99%E5%85%A5CSV%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 写入CSV文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81JSON"><span class="toc-number">1.3.</span> <span class="toc-text">三、JSON</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AF%BB%E5%8F%96JSON%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 读取JSON文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%86%99%E5%85%A5JSON%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 写入JSON文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Parquet"><span class="toc-number">1.4.</span> <span class="toc-text">四、Parquet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%AF%BB%E5%8F%96Parquet%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 读取Parquet文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%86%99%E5%85%A5Parquet%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.2.</span> <span class="toc-text">2.2 写入Parquet文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE-1"><span class="toc-number">1.4.3.</span> <span class="toc-text">2.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81ORC"><span class="toc-number">1.5.</span> <span class="toc-text">五、ORC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%AF%BB%E5%8F%96ORC%E6%96%87%E4%BB%B6"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 读取ORC文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%86%99%E5%85%A5ORC%E6%96%87%E4%BB%B6"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 写入ORC文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81SQL-Databases"><span class="toc-number">1.6.</span> <span class="toc-text">六、SQL Databases</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 读取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 写入数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81Text"><span class="toc-number">1.7.</span> <span class="toc-text">七、Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%AF%BB%E5%8F%96Text%E6%95%B0%E6%8D%AE"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 读取Text数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E5%86%99%E5%85%A5Text%E6%95%B0%E6%8D%AE"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 写入Text数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7"><span class="toc-number">1.8.</span> <span class="toc-text">八、数据读写高级特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E5%B9%B6%E8%A1%8C%E8%AF%BB"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 并行读</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E5%B9%B6%E8%A1%8C%E5%86%99"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 并行写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 分区写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%88%86%E6%A1%B6%E5%86%99%E5%85%A5"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.3 分桶写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%AE%A1%E7%90%86"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 文件大小管理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%E9%99%84%E5%BD%95"><span class="toc-number">1.9.</span> <span class="toc-text">九、可选配置附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-CSV%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 CSV读写可选配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-JSON%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 JSON读写可选配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 数据库读写可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.10.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        SparkSQL外部数据源
    </h1>



    <div class="meta">
      <!-- <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Likanug's Blog</span>
      </span>
            |&nbsp; -->
    <i class="fas fa-eye"></i>
      <span id="busuanzi_container_page_pv" class="article-count-a-span">
          <span id="busuanzi_value_page_pv">
          </span>
      </span>&nbsp;|
	
    <div class="postdate">
      
        <time datetime="2021-03-13T10:31:55.000Z" itemprop="datePublished">2021-03-13</time>
        
        (Updated: <time datetime="2021-03-13T11:35:51.175Z" itemprop="dateModified">2021-03-13</time>)
        
      
    </div>

  
    
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="../../../../categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
    </div>


    
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="../../../../tags/Spark/" rel="tag">Spark</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Spark-SQL-外部数据源"><a href="#Spark-SQL-外部数据源" class="headerlink" title="Spark SQL 外部数据源"></a>Spark SQL 外部数据源</h1><nav>
<a href="#一简介">一、简介</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#11-多数据源支持">1.1 多数据源支持</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#12-读数据格式">1.2 读数据格式</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#13-写数据格式">1.3 写数据格式</a><br/>
<a href="#二CSV">二、CSV</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#21-读取CSV文件">2.1 读取CSV文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#22-写入CSV文件">2.2 写入CSV文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#23-可选配置">2.3 可选配置</a><br/>
<a href="#三JSON">三、JSON</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#31-读取JSON文件">3.1 读取JSON文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#32-写入JSON文件">3.2 写入JSON文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#33-可选配置">3.3 可选配置</a><br/>
<a href="#四Parquet">四、Parquet</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-读取Parquet文件">4.1 读取Parquet文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#22-写入Parquet文件">2.2 写入Parquet文件</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#23-可选配置">2.3 可选配置</a><br/>
<a href="#五ORC">五、ORC </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#51-读取ORC文件">5.1 读取ORC文件 </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-写入ORC文件">4.2 写入ORC文件</a><br/>
<a href="#六SQL-Databases">六、SQL Databases </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#61-读取数据">6.1 读取数据</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#62-写入数据">6.2 写入数据</a><br/>
<a href="#七Text">七、Text </a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#71-读取Text数据">7.1 读取Text数据</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#72-写入Text数据">7.2 写入Text数据</a><br/>
<a href="#八数据读写高级特性">八、数据读写高级特性</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#81-并行读">8.1 并行读</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#82-并行写">8.2 并行写</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#83-分区写入">8.3 分区写入</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#83-分桶写入">8.3 分桶写入</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#85-文件大小管理">8.5 文件大小管理</a><br/>
<a href="#九可选配置附录">九、可选配置附录</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#91-CSV读写可选配置">9.1 CSV读写可选配置</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#92-JSON读写可选配置">9.2 JSON读写可选配置</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#93-数据库读写可选配置">9.3 数据库读写可选配置</a><br/>
</nav>

<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-多数据源支持"><a href="#1-1-多数据源支持" class="headerlink" title="1.1 多数据源支持"></a>1.1 多数据源支持</h3><p>Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。</p>
<ul>
<li>CSV</li>
<li>JSON</li>
<li>Parquet</li>
<li>ORC</li>
<li>JDBC/ODBC connections</li>
<li>Plain-text files</li>
</ul>
<blockquote>
<p>注：以下所有测试文件均可从本仓库的<a target="_blank" rel="noopener" href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录进行下载</p>
</blockquote>
<h3 id="1-2-读数据格式"><a href="#1-2-读数据格式" class="headerlink" title="1.2 读数据格式"></a>1.2 读数据格式</h3><p>所有读取 API 遵循以下调用格式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameReader</span>.format(...).option(<span class="string">&quot;key&quot;</span>, <span class="string">&quot;value&quot;</span>).schema(...).load()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 示例</span></span><br><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)          <span class="comment">// 读取模式</span></span><br><span class="line">.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)       <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)   <span class="comment">// 文件路径</span></span><br><span class="line">.schema(someSchema)                  <span class="comment">// 使用预定义的 schema      </span></span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>

<p>读取模式有以下三种可选项：</p>
<table>
<thead>
<tr>
<th>读模式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td><code>permissive</code></td>
<td>当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中</td>
</tr>
<tr>
<td><code>dropMalformed</code></td>
<td>删除格式不正确的行</td>
</tr>
<tr>
<td><code>failFast</code></td>
<td>遇到格式不正确的数据时立即失败</td>
</tr>
</tbody></table>
<h3 id="1-3-写数据格式"><a href="#1-3-写数据格式" class="headerlink" title="1.3 写数据格式"></a>1.3 写数据格式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 格式</span></span><br><span class="line"><span class="type">DataFrameWriter</span>.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br><span class="line"></span><br><span class="line"><span class="comment">//示例</span></span><br><span class="line">dataframe.write.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;OVERWRITE&quot;</span>)         <span class="comment">//写模式</span></span><br><span class="line">.option(<span class="string">&quot;dateFormat&quot;</span>, <span class="string">&quot;yyyy-MM-dd&quot;</span>)  <span class="comment">//日期格式</span></span><br><span class="line">.option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>

<p>写数据模式有以下四种可选项：</p>
<table>
<thead>
<tr>
<th align="left">Scala/Java</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code></td>
<td align="left">如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">数据以追加的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Overwrite</code></td>
<td align="left">数据以覆盖的方式写入</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Ignore</code></td>
<td align="left">如果给定的路径已经存在文件，则不做任何操作</td>
</tr>
</tbody></table>
<br/>

<h2 id="二、CSV"><a href="#二、CSV" class="headerlink" title="二、CSV"></a>二、CSV</h2><p>CSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。</p>
<h3 id="2-1-读取CSV文件"><a href="#2-1-读取CSV文件" class="headerlink" title="2.1 读取CSV文件"></a>2.1 读取CSV文件</h3><p>自动推断类型读取读取示例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;false&quot;</span>)        <span class="comment">// 文件中的第一行是否为列的名称</span></span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)      <span class="comment">// 是否快速失败</span></span><br><span class="line">.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)   <span class="comment">// 是否自动推断 schema</span></span><br><span class="line">.load(<span class="string">&quot;/usr/file/csv/dept.csv&quot;</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<p>使用预定义类型：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>,<span class="type">LongType</span>&#125;</span><br><span class="line"><span class="comment">//预定义数据格式</span></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;deptno&quot;</span>, <span class="type">LongType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;dname&quot;</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>),</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;loc&quot;</span>, <span class="type">StringType</span>,nullable = <span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)</span><br><span class="line">.schema(myManualSchema)</span><br><span class="line">.load(<span class="string">&quot;/usr/file/csv/dept.csv&quot;</span>)</span><br><span class="line">.show()</span><br></pre></td></tr></table></figure>

<h3 id="2-2-写入CSV文件"><a href="#2-2-写入CSV文件" class="headerlink" title="2.2 写入CSV文件"></a>2.2 写入CSV文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;csv&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/csv/dept2&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>也可以指定具体的分隔符：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;csv&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>).save(<span class="string">&quot;/tmp/csv/dept2&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-可选配置"><a href="#2-3-可选配置" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.1 小节。</p>
<br/>

<h2 id="三、JSON"><a href="#三、JSON" class="headerlink" title="三、JSON"></a>三、JSON</h2><h3 id="3-1-读取JSON文件"><a href="#3-1-读取JSON文件" class="headerlink" title="3.1 读取JSON文件"></a>3.1 读取JSON文件</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(&quot;json&quot;).option(&quot;mode&quot;, &quot;FAILFAST&quot;).load(&quot;/usr/file/json/dept.json&quot;).show(5)</span><br></pre></td></tr></table></figure>

<p>需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 <code>multiLine</code> 为 <code>true</code> 来进行更改，其默认值为 <code>false</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认支持单行</span></span><br><span class="line">&#123;<span class="attr">&quot;DEPTNO&quot;</span>: <span class="number">10</span>,<span class="attr">&quot;DNAME&quot;</span>: <span class="string">&quot;ACCOUNTING&quot;</span>,<span class="attr">&quot;LOC&quot;</span>: <span class="string">&quot;NEW YORK&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//默认不支持多行</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;DEPTNO&quot;</span>: <span class="number">10</span>,</span><br><span class="line">  <span class="attr">&quot;DNAME&quot;</span>: <span class="string">&quot;ACCOUNTING&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;LOC&quot;</span>: <span class="string">&quot;NEW YORK&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-写入JSON文件"><a href="#3-2-写入JSON文件" class="headerlink" title="3.2 写入JSON文件"></a>3.2 写入JSON文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;json&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/json/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-3-可选配置"><a href="#3-3-可选配置" class="headerlink" title="3.3 可选配置"></a>3.3 可选配置</h3><p>为节省主文篇幅，所有读写配置项见文末 9.2 小节。</p>
<br/>

<h2 id="四、Parquet"><a href="#四、Parquet" class="headerlink" title="四、Parquet"></a>四、Parquet</h2><p> Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。</p>
<h3 id="4-1-读取Parquet文件"><a href="#4-1-读取Parquet文件" class="headerlink" title="4.1 读取Parquet文件"></a>4.1 读取Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;parquet&quot;</span>).load(<span class="string">&quot;/usr/file/parquet/dept.parquet&quot;</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-写入Parquet文件"><a href="#2-2-写入Parquet文件" class="headerlink" title="2.2 写入Parquet文件"></a>2.2 写入Parquet文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/parquet/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-可选配置-1"><a href="#2-3-可选配置-1" class="headerlink" title="2.3 可选配置"></a>2.3 可选配置</h3><p>Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个：</p>
<table>
<thead>
<tr>
<th>读写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Write</td>
<td>compression or codec</td>
<td>None,<br/>uncompressed,<br/>bzip2,<br/>deflate, gzip,<br/>lz4, or snappy</td>
<td>None</td>
<td>压缩文件格式</td>
</tr>
<tr>
<td>Read</td>
<td>mergeSchema</td>
<td>true, false</td>
<td>取决于配置项 <code>spark.sql.parquet.mergeSchema</code></td>
<td>当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。</td>
</tr>
</tbody></table>
<blockquote>
<p>更多可选配置可以参阅官方文档：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></p>
</blockquote>
<br/>

<h2 id="五、ORC"><a href="#五、ORC" class="headerlink" title="五、ORC"></a>五、ORC</h2><p>ORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。</p>
<h3 id="5-1-读取ORC文件"><a href="#5-1-读取ORC文件" class="headerlink" title="5.1 读取ORC文件"></a>5.1 读取ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;orc&quot;</span>).load(<span class="string">&quot;/usr/file/orc/dept.orc&quot;</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-2-写入ORC文件"><a href="#4-2-写入ORC文件" class="headerlink" title="4.2 写入ORC文件"></a>4.2 写入ORC文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csvFile.write.format(<span class="string">&quot;orc&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/spark/orc/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="六、SQL-Databases"><a href="#六、SQL-Databases" class="headerlink" title="六、SQL Databases"></a>六、SQL Databases</h2><p>Spark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 <code>jars</code> 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 <code>mysql-connector-java-x.x.x.jar</code> 上传到 <code>jars</code> 目录下。</p>
<h3 id="6-1-读取数据"><a href="#6-1-读取数据" class="headerlink" title="6.1 读取数据"></a>6.1 读取数据</h3><p>读取全表数据示例如下，这里的 <code>help_keyword</code> 是 mysql 内置的字典表，只有 <code>help_keyword_id</code> 和 <code>name</code> 两个字段。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)            <span class="comment">//驱动</span></span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)   <span class="comment">//数据库地址</span></span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;help_keyword&quot;</span>)                    <span class="comment">//表名</span></span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;root&quot;</span>).load().show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>从查询结果读取数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pushDownQuery = <span class="string">&quot;&quot;</span><span class="string">&quot;(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, pushDownQuery)</span><br><span class="line">.load().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|             <span class="number">10</span>|      <span class="type">ALTER</span>|</span><br><span class="line">|             <span class="number">11</span>|    <span class="type">ANALYSE</span>|</span><br><span class="line">|             <span class="number">12</span>|    <span class="type">ANALYZE</span>|</span><br><span class="line">|             <span class="number">13</span>|        <span class="type">AND</span>|</span><br><span class="line">|             <span class="number">14</span>|    <span class="type">ARCHIVE</span>|</span><br><span class="line">|             <span class="number">15</span>|       <span class="type">AREA</span>|</span><br><span class="line">|             <span class="number">16</span>|         <span class="type">AS</span>|</span><br><span class="line">|             <span class="number">17</span>|   <span class="type">ASBINARY</span>|</span><br><span class="line">|             <span class="number">18</span>|        <span class="type">ASC</span>|</span><br><span class="line">|             <span class="number">19</span>|     <span class="type">ASTEXT</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>

<p>也可以使用如下的写法进行数据的过滤：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> props = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">props.setProperty(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">props.setProperty(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> predicates = <span class="type">Array</span>(<span class="string">&quot;help_keyword_id &lt; 10  OR name = &#x27;WHEN&#x27;&quot;</span>)   <span class="comment">//指定数据过滤条件</span></span><br><span class="line">spark.read.jdbc(<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>, <span class="string">&quot;help_keyword&quot;</span>, predicates, props).show() </span><br><span class="line"></span><br><span class="line"><span class="comment">//输出：</span></span><br><span class="line">+---------------+-----------+</span><br><span class="line">|help_keyword_id|       name|</span><br><span class="line">+---------------+-----------+</span><br><span class="line">|              <span class="number">0</span>|         &lt;&gt;|</span><br><span class="line">|              <span class="number">1</span>|     <span class="type">ACTION</span>|</span><br><span class="line">|              <span class="number">2</span>|        <span class="type">ADD</span>|</span><br><span class="line">|              <span class="number">3</span>|<span class="type">AES_DECRYPT</span>|</span><br><span class="line">|              <span class="number">4</span>|<span class="type">AES_ENCRYPT</span>|</span><br><span class="line">|              <span class="number">5</span>|      <span class="type">AFTER</span>|</span><br><span class="line">|              <span class="number">6</span>|    <span class="type">AGAINST</span>|</span><br><span class="line">|              <span class="number">7</span>|  <span class="type">AGGREGATE</span>|</span><br><span class="line">|              <span class="number">8</span>|  <span class="type">ALGORITHM</span>|</span><br><span class="line">|              <span class="number">9</span>|        <span class="type">ALL</span>|</span><br><span class="line">|            <span class="number">604</span>|       <span class="type">WHEN</span>|</span><br><span class="line">+---------------+-----------+</span><br></pre></td></tr></table></figure>

<p>可以使用 <code>numPartitions</code> 指定读取数据的并行度：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">option(<span class="string">&quot;numPartitions&quot;</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> colName = <span class="string">&quot;help_keyword_id&quot;</span>   <span class="comment">//用于判断上下界的列</span></span><br><span class="line"><span class="keyword">val</span> lowerBound = <span class="number">300</span>L    <span class="comment">//下界</span></span><br><span class="line"><span class="keyword">val</span> upperBound = <span class="number">500</span>L    <span class="comment">//上界</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = <span class="number">10</span>   <span class="comment">//分区综述</span></span><br><span class="line"><span class="keyword">val</span> jdbcDf = spark.read.jdbc(<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>,<span class="string">&quot;help_keyword&quot;</span>,</span><br><span class="line">                             colName,lowerBound,upperBound,numPartitions,props)</span><br></pre></td></tr></table></figure>

<p>想要验证分区内容，可以使用 <code>mapPartitionsWithIndex</code> 这个算子，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">        buffer.append(index + <span class="string">&quot;分区:&quot;</span> + iterator.next())</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.toIterator</span><br><span class="line">&#125;).foreach(println)</span><br></pre></td></tr></table></figure>

<p>执行结果如下：<code>help_keyword</code> 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。</p>
<div align="center"> <img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-mysql-分区上下限.png"/> </div>

<h3 id="6-2-写入数据"><a href="#6-2-写入数据" class="headerlink" title="6.2 写入数据"></a>6.2 写入数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">df.write</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://127.0.0.1:3306/mysql&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>).option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;emp&quot;</span>)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="七、Text"><a href="#七、Text" class="headerlink" title="七、Text"></a>七、Text</h2><p>Text 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下：</p>
<h3 id="7-1-读取Text数据"><a href="#7-1-读取Text数据" class="headerlink" title="7.1 读取Text数据"></a>7.1 读取Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.textFile(<span class="string">&quot;/usr/file/txt/dept.txt&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="7-2-写入Text数据"><a href="#7-2-写入Text数据" class="headerlink" title="7.2 写入Text数据"></a>7.2 写入Text数据</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.text(<span class="string">&quot;/tmp/spark/txt/dept&quot;</span>)</span><br></pre></td></tr></table></figure>

<br/>

<h2 id="八、数据读写高级特性"><a href="#八、数据读写高级特性" class="headerlink" title="八、数据读写高级特性"></a>八、数据读写高级特性</h2><h3 id="8-1-并行读"><a href="#8-1-并行读" class="headerlink" title="8.1 并行读"></a>8.1 并行读</h3><p>多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。</p>
<h3 id="8-2-并行写"><a href="#8-2-并行写" class="headerlink" title="8.2 并行写"></a>8.2 并行写</h3><p>写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。</p>
<h3 id="8-3-分区写入"><a href="#8-3-分区写入" class="headerlink" title="8.3 分区写入"></a>8.3 分区写入</h3><p>分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 <code>partitionBy</code> 指定的分区和 RDD 中分区不是一个概念：这里的<strong>分区表现为输出目录的子目录</strong>，数据分别存储在对应的子目录中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/usr/file/json/emp.json&quot;</span>)</span><br><span class="line">df.write.mode(<span class="string">&quot;overwrite&quot;</span>).partitionBy(<span class="string">&quot;deptno&quot;</span>).save(<span class="string">&quot;/tmp/spark/partitions&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。</p>
<div align="center"> <img src="https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-分区.png"/> </div>

<h3 id="8-3-分桶写入"><a href="#8-3-分桶写入" class="headerlink" title="8.3 分桶写入"></a>8.3 分桶写入</h3><p>分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numberBuckets = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> columnToBucketBy = <span class="string">&quot;empno&quot;</span></span><br><span class="line">df.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">.bucketBy(numberBuckets, columnToBucketBy).saveAsTable(<span class="string">&quot;bucketedFiles&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="8-5-文件大小管理"><a href="#8-5-文件大小管理" class="headerlink" title="8.5 文件大小管理"></a>8.5 文件大小管理</h3><p>如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。</p>
<p>在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 <code>maxRecordsPerFile</code> 参数，它允许你通过控制写入文件的记录数来控制文件大小。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Spark 将确保文件最多包含 5000 条记录</span></span><br><span class="line">df.write.option(“maxRecordsPerFile”, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>

<br>

<h2 id="九、可选配置附录"><a href="#九、可选配置附录" class="headerlink" title="九、可选配置附录"></a>九、可选配置附录</h2><h3 id="9-1-CSV读写可选配置"><a href="#9-1-CSV读写可选配置" class="headerlink" title="9.1 CSV读写可选配置"></a>9.1 CSV读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Both</td>
<td>seq</td>
<td>任意字符</td>
<td><code>,</code>(逗号)</td>
<td>分隔符</td>
</tr>
<tr>
<td>Both</td>
<td>header</td>
<td>true, false</td>
<td>false</td>
<td>文件中的第一行是否为列的名称。</td>
</tr>
<tr>
<td>Read</td>
<td>escape</td>
<td>任意字符</td>
<td>\</td>
<td>转义字符</td>
</tr>
<tr>
<td>Read</td>
<td>inferSchema</td>
<td>true, false</td>
<td>false</td>
<td>是否自动推断列类型</td>
</tr>
<tr>
<td>Read</td>
<td>ignoreLeadingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值前面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>ignoreTrailingWhiteSpace</td>
<td>true, false</td>
<td>false</td>
<td>是否跳过值后面的空格</td>
</tr>
<tr>
<td>Both</td>
<td>nullValue</td>
<td>任意字符</td>
<td>“”</td>
<td>声明文件中哪个字符表示空值</td>
</tr>
<tr>
<td>Both</td>
<td>nanValue</td>
<td>任意字符</td>
<td>NaN</td>
<td>声明哪个值表示 NaN 或者缺省值</td>
</tr>
<tr>
<td>Both</td>
<td>positiveInf</td>
<td>任意字符</td>
<td>Inf</td>
<td>正无穷</td>
</tr>
<tr>
<td>Both</td>
<td>negativeInf</td>
<td>任意字符</td>
<td>-Inf</td>
<td>负无穷</td>
</tr>
<tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br/>uncompressed,<br/>bzip2, deflate,<br/>gzip, lz4, or<br/>snappy</td>
<td>none</td>
<td>文件压缩格式</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 <br/>SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
<td>日期格式</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 <br/>SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
<td>时间戳格式</td>
</tr>
<tr>
<td>Read</td>
<td>maxColumns</td>
<td>任意整数</td>
<td>20480</td>
<td>声明文件中的最大列数</td>
</tr>
<tr>
<td>Read</td>
<td>maxCharsPerColumn</td>
<td>任意整数</td>
<td>1000000</td>
<td>声明一个列中的最大字符数。</td>
</tr>
<tr>
<td>Read</td>
<td>escapeQuotes</td>
<td>true, false</td>
<td>true</td>
<td>是否应该转义行中的引号。</td>
</tr>
<tr>
<td>Read</td>
<td>maxMalformedLogPerPartition</td>
<td>任意整数</td>
<td>10</td>
<td>声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取</td>
</tr>
<tr>
<td>Write</td>
<td>quoteAll</td>
<td>true, false</td>
<td>false</td>
<td>指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
<td>是否允许每条完整记录跨域多行</td>
</tr>
</tbody></table>
<h3 id="9-2-JSON读写可选配置"><a href="#9-2-JSON读写可选配置" class="headerlink" title="9.2 JSON读写可选配置"></a>9.2 JSON读写可选配置</h3><table>
<thead>
<tr>
<th>读\写操作</th>
<th>配置项</th>
<th>可选值</th>
<th>默认值</th>
</tr>
</thead>
<tbody><tr>
<td>Both</td>
<td>compression or codec</td>
<td>None,<br/>uncompressed,<br/>bzip2, deflate,<br/>gzip, lz4, or<br/>snappy</td>
<td>none</td>
</tr>
<tr>
<td>Both</td>
<td>dateFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MM-dd</td>
</tr>
<tr>
<td>Both</td>
<td>timestampFormat</td>
<td>任何能转换为 Java 的 SimpleDataFormat 的字符串</td>
<td>yyyy-MMdd’T’HH:mm:ss.SSSZZ</td>
</tr>
<tr>
<td>Read</td>
<td>primitiveAsString</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowComments</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowUnquotedFieldNames</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowSingleQuotes</td>
<td>true, false</td>
<td>true</td>
</tr>
<tr>
<td>Read</td>
<td>allowNumericLeadingZeros</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>allowBackslashEscapingAnyCharacter</td>
<td>true, false</td>
<td>false</td>
</tr>
<tr>
<td>Read</td>
<td>columnNameOfCorruptRecord</td>
<td>true, false</td>
<td>Value of spark.sql.column&amp;NameOf</td>
</tr>
<tr>
<td>Read</td>
<td>multiLine</td>
<td>true, false</td>
<td>false</td>
</tr>
</tbody></table>
<h3 id="9-3-数据库读写可选配置"><a href="#9-3-数据库读写可选配置" class="headerlink" title="9.3 数据库读写可选配置"></a>9.3 数据库读写可选配置</h3><table>
<thead>
<tr>
<th>属性名称</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>url</td>
<td>数据库地址</td>
</tr>
<tr>
<td>dbtable</td>
<td>表名称</td>
</tr>
<tr>
<td>driver</td>
<td>数据库驱动</td>
</tr>
<tr>
<td>partitionColumn,<br/>lowerBound, upperBoun</td>
<td>分区总数，上界，下界</td>
</tr>
<tr>
<td>numPartitions</td>
<td>可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。</td>
</tr>
<tr>
<td>fetchsize</td>
<td>每次往返要获取多少行数据。此选项仅适用于读取数据。</td>
</tr>
<tr>
<td>batchsize</td>
<td>每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。</td>
</tr>
<tr>
<td>isolationLevel</td>
<td>事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。<br/>默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。</td>
</tr>
<tr>
<td>createTableOptions</td>
<td>写入数据时自定义创建表的相关配置</td>
</tr>
<tr>
<td>createTableColumnTypes</td>
<td>写入数据时自定义创建列的列类型</td>
</tr>
</tbody></table>
<blockquote>
<p>数据库读写更多配置可以参阅官方文档：<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a></p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li>Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 </li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/sql-data-sources.html">https://spark.apache.org/docs/latest/sql-data-sources.html</a></li>
</ol>

  </div>
  <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
</article>

    <div class="blog-post-comments">
        
            
            <div class="vcomment"></div>
        
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="../../../../index.html">Home</a></li>
         
          <li><a href="../../../../archives/">Writing</a></li>
         
          <li><a href="../../../../about/">About</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL-%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL 外部数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">一、简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E6%94%AF%E6%8C%81"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1 多数据源支持</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2 读数据格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%86%99%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3 写数据格式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81CSV"><span class="toc-number">1.2.</span> <span class="toc-text">二、CSV</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%BB%E5%8F%96CSV%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 读取CSV文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%86%99%E5%85%A5CSV%E6%96%87%E4%BB%B6"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 写入CSV文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text">2.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81JSON"><span class="toc-number">1.3.</span> <span class="toc-text">三、JSON</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AF%BB%E5%8F%96JSON%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.1.</span> <span class="toc-text">3.1 读取JSON文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%86%99%E5%85%A5JSON%E6%96%87%E4%BB%B6"><span class="toc-number">1.3.2.</span> <span class="toc-text">3.2 写入JSON文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Parquet"><span class="toc-number">1.4.</span> <span class="toc-text">四、Parquet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%AF%BB%E5%8F%96Parquet%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.1.</span> <span class="toc-text">4.1 读取Parquet文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%86%99%E5%85%A5Parquet%E6%96%87%E4%BB%B6"><span class="toc-number">1.4.2.</span> <span class="toc-text">2.2 写入Parquet文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE-1"><span class="toc-number">1.4.3.</span> <span class="toc-text">2.3 可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81ORC"><span class="toc-number">1.5.</span> <span class="toc-text">五、ORC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%AF%BB%E5%8F%96ORC%E6%96%87%E4%BB%B6"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.1 读取ORC文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%86%99%E5%85%A5ORC%E6%96%87%E4%BB%B6"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 写入ORC文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81SQL-Databases"><span class="toc-number">1.6.</span> <span class="toc-text">六、SQL Databases</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">1.6.1.</span> <span class="toc-text">6.1 读取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-number">1.6.2.</span> <span class="toc-text">6.2 写入数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81Text"><span class="toc-number">1.7.</span> <span class="toc-text">七、Text</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%AF%BB%E5%8F%96Text%E6%95%B0%E6%8D%AE"><span class="toc-number">1.7.1.</span> <span class="toc-text">7.1 读取Text数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E5%86%99%E5%85%A5Text%E6%95%B0%E6%8D%AE"><span class="toc-number">1.7.2.</span> <span class="toc-text">7.2 写入Text数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7"><span class="toc-number">1.8.</span> <span class="toc-text">八、数据读写高级特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E5%B9%B6%E8%A1%8C%E8%AF%BB"><span class="toc-number">1.8.1.</span> <span class="toc-text">8.1 并行读</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E5%B9%B6%E8%A1%8C%E5%86%99"><span class="toc-number">1.8.2.</span> <span class="toc-text">8.2 并行写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5"><span class="toc-number">1.8.3.</span> <span class="toc-text">8.3 分区写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%88%86%E6%A1%B6%E5%86%99%E5%85%A5"><span class="toc-number">1.8.4.</span> <span class="toc-text">8.3 分桶写入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-5-%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F%E7%AE%A1%E7%90%86"><span class="toc-number">1.8.5.</span> <span class="toc-text">8.5 文件大小管理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE%E9%99%84%E5%BD%95"><span class="toc-number">1.9.</span> <span class="toc-text">九、可选配置附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-CSV%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.1.</span> <span class="toc-text">9.1 CSV读写可选配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-JSON%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.2.</span> <span class="toc-text">9.2 JSON读写可选配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%BB%E5%86%99%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-number">1.9.3.</span> <span class="toc-text">9.3 数据库读写可选配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.10.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&text=SparkSQL外部数据源"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&is_video=false&description=SparkSQL外部数据源"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=SparkSQL外部数据源&body=Check out this article: https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&title=SparkSQL外部数据源"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&name=SparkSQL外部数据源&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.likanug.top/2021/03/13/SparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90/&t=SparkSQL外部数据源"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2021 Likanug
    <a href="http://www.beian.miit.gov.cn" rel="external nofollow noopener noreferrer" target="_blank"></a>
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="../../../../index.html">Home</a></li>
         
          <li><a href="../../../../archives/">Writing</a></li>
         
          <li><a href="../../../../about/">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

        <!-- jquery -->

<script src="../../../../lib/jquery/jquery.min.js"></script>


<script src="../../../../lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>

<!-- clipboard -->

  
<script src="../../../../lib/clipboard/clipboard.min.js"></script>

  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="../../../../js/main.js"></script>

<!-- search -->

<!-- Valine Comments -->

  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
  <script type="text/javascript">
      var notify = 'false' == true ? true : false;
      var verify = 'false' == true ? true : false;
      var GUEST_INFO = ['nick','mail','link'];
      var guest_info = 'nick,mail,link'.split(',').filter(function(item){
        return GUEST_INFO.indexOf(item) > -1
      });
      guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
      new Valine({
          el: '.vcomment',
          notify: notify,
          verify: verify,
          appId: "lJPojt39F66YItEiORkKhHp1-9Nh9j0Va",
          appKey: "L3RsjYdcE6GdDUciAgeyjlfn",
          avatar:"mm",
          placeholder: "Just go go",
          guest_info:guest_info,
          pageSize:"10"
      })
  </script>

<!-- Google Analytics -->

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?2e6da3c375c8a87f5b664cea6d4cb29c";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>


    </div>
</body>
</html>
